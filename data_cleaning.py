# -*- coding: utf-8 -*-
"""Data_Cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14fjt_6_rpeMgF3QO8bUL2LL_mZvvs41t
"""

pip install emoji

import pandas as pd
import numpy as np
import re
import emoji
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Download NLTK data
import nltk
nltk.download('stopdwords')
nltk.download('punkt')
nltk.download('wordnet')

# Function to remove HTML tags
def remove_html_tags(text):
    return BeautifulSoup(text, "html.parser").get_text()

# Function to remove emojis
def remove_emojis(text):
    return emoji.replace_emoji(text, replace='')

# Function to normalize contractions
def normalize_contractions(text):
    contractions = {
        "can't": "cannot",
        "won't": "will not",
        "n't": " not",
        "'re": " are",
        "'s": " is",
        "'d": " would",
        "'ll": " will",
        "'t": " not",
        "'ve": " have",
        "'m": " am"
    }
    for contraction, full_form in contractions.items():
        text = re.sub(contraction, full_form, text)
    return text

# Function to remove special characters and normalize whitespace
def clean_special_characters(text):
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'[^A-Za-z0-9\s]', '', text)  # Remove special characters
    text = re.sub(r'\s+', ' ', text)  # Normalize whitespace
    return text.strip()

# Function to remove stop words
def remove_stop_words(text):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]
    return ' '.join(filtered_text)

# Function to lemmatize text
def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    word_tokens = word_tokenize(text)
    lemmatized_text = [lemmatizer.lemmatize(word) for word in word_tokens]
    return ' '.join(lemmatized_text)

# Advanced cleaning function for text data
def clean_text(text):
    text = remove_html_tags(text)
    text = remove_emojis(text)
    text = normalize_contractions(text)
    text = clean_special_characters(text)
    text = remove_stop_words(text)
    text = lemmatize_text(text)
    return text.lower()

# Load the data
df = pd.read_csv('consolidated_metadata_rest.csv')

df

# Remove duplicates
df.drop_duplicates(inplace=True)

df

# Handle missing values
df.dropna(subset=['subreddit'], inplace=True)
# df['description'].fillna('unknown', inplace=True)
df['selftext'].fillna('', inplace=True)

df

import nltk
nltk.download('stopwords')

# Standardize and clean text data
# df['description'] = df['description'].apply(clean_text)
df['selftext'] = df['selftext'].apply(clean_text)

df

# Standardize date format
df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')

df

# Convert data types
df['upvotes'] = df['upvotes'].astype(int)
df['downvotes'] = df['downvotes'].astype(int)

df

# Detect and handle outliers in upvotes and downvotes (example: using IQR)
Q1_upvotes = df['upvotes'].quantile(0.25)
Q3_upvotes = df['upvotes'].quantile(0.75)
IQR_upvotes = Q3_upvotes - Q1_upvotes
df = df[~((df['upvotes'] < (Q1_upvotes - 1.5 * IQR_upvotes)) | (df['upvotes'] > (Q3_upvotes + 1.5 * IQR_upvotes)))]

Q1_downvotes = df['downvotes'].quantile(0.25)
Q3_downvotes = df['downvotes'].quantile(0.75)
IQR_downvotes = Q3_downvotes - Q1_downvotes
df = df[~((df['downvotes'] < (Q1_downvotes - 1.5 * IQR_downvotes)) | (df['downvotes'] > (Q3_downvotes + 1.5 * IQR_downvotes)))]

# Remove irrelevant fields (if any)
df = df[['id', 'title', 'selftext', 'upvotes', 'downvotes', 'created_utc', 'author','subreddit']]

# Filter out low engagement posts (example: less than 10 upvotes)
df = df[df['upvotes'] >= 10]

# # Save the cleaned data
# df.to_json('/mnt/data/cleaned_subreddit_posts.json', orient='records', lines=True)

df

df.to_csv('Cleaned_posts.csv')

subreddits_df=pd.read_csv('Cleaned_posts.csv')

subreddits_df

# Group posts by subreddit_id
grouped_posts = subreddits_df.groupby('subreddit')

print(grouped_posts)