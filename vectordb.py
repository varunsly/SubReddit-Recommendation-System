# -*- coding: utf-8 -*-
"""VectorDB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bK4-jPa7nzIFVNTfRS3TZm5Maxf1CcqN
"""

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

import sqlite3
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import numpy as np
from tqdm import tqdm
import faiss
import multiprocessing as mp
import re
import sqlite3
import torch
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk
import pickle

# pip install faiss-gpu

# Download NLTK resources (run this once)
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize stop words and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Initialize the SentenceTransformer model (uses PyTorch)
model_name = 'all-mpnet-base-v2'  # You can choose other models as well
model = SentenceTransformer(model_name)

"""## Database Connection and Data Loading"""

### Step 1: Database Connection and Data Loading ###

def connect_to_db(db_name):
    try:
        conn = sqlite3.connect(db_name)
        return conn
    except sqlite3.Error as e:
        print(f"Error connecting to database: {e}")
        return None

def load_subreddit_data(conn):
    try:
        query = """
        SELECT id, name, description, subscribers, category, created_utc
        FROM subreddits
        WHERE description IS NOT NULL AND description != ''
        """
        subreddits_df = pd.read_sql_query(query, conn)
        return subreddits_df
    except Exception as e:
        print(f"Error loading subreddit data: {e}")
        return None
conn = connect_to_db('/umbc/rs/gokhale/users/vmagotr1/varun/data/Reddit Data/reddit_data_new.db')
subreddits_df = load_subreddit_data(conn)

# Execute the ALTER TABLE command to add the rich_text column
def add_rich_text_column(conn):
    try:
        conn.execute("ALTER TABLE subreddits ADD COLUMN rich_text TEXT;")
        conn.commit()  # Commit the changes to the database
        print("Column 'rich_text' added successfully.")
    except Exception as e:
        print(f"Error adding 'rich_text' column: {e}")

# Call the function to add the column
add_rich_text_column(conn)

"""## Data Preparation"""

def get_top_posts_batch(conn, subreddit_ids, limit=5):
    # Convert the subreddit_ids into a comma-separated string for the SQL IN clause
    query = f"""
    SELECT subreddit_id, title, selftext, upvotes, downvotes
    FROM posts
    WHERE subreddit_id IN ({','.join(['?']*len(subreddit_ids))})
    ORDER BY upvotes DESC
    LIMIT ?
    """
    try:
        top_posts = pd.read_sql_query(query, conn, params=(*subreddit_ids, limit))
        return top_posts
    except Exception as e:
        print(f"Error fetching top posts for subreddits: {e}")
        return pd.DataFrame()

def create_rich_text_for_batch(subreddit_df, top_posts_df):
    rich_texts = []

    for idx, row in subreddit_df.iterrows():
        base_text = f"Subreddit: {row['name']}\n"
        base_text += f"Description: {row['description']}\n"
        base_text += f"Subscribers: {row['subscribers']}\n"
        base_text += f"Category: {row['category']}\n"

        # Handle the created_utc field
        try:
            created_date = pd.to_datetime(row['created_utc'], unit='s').date()
        except ValueError:
            created_date = pd.to_datetime(row['created_utc']).date()
        base_text += f"Created: {created_date}\n"
        base_text += "Top Posts:\n"

        # Filtering top posts specific to the current subreddit
        subreddit_top_posts = top_posts_df[top_posts_df['subreddit_id'] == row['id']]

        for _, post in subreddit_top_posts.iterrows():
            base_text += f"- Title: {post['title']}\n"
            if pd.notna(post['selftext']) and post['selftext'].strip() != '':
                content_preview = post['selftext'][:100].replace('\n', ' ').strip()
                base_text += f"  Content: {content_preview}...\n"
            base_text += f"  Upvotes: {post['upvotes']}, Downvotes: {post['downvotes']}\n"

        rich_texts.append(base_text)

    return rich_texts


batch_size = 1000 # Batch processing size (tune this based on available memory/performance)
subreddits_df['rich_text'] = None

def update_rich_text_in_db(conn, subreddit_id, rich_text):
    update_query = """
    UPDATE subreddits
    SET rich_text = ?
    WHERE id = ?
    """
    try:
        conn.execute(update_query, (rich_text, subreddit_id))
        conn.commit()  # Commit the transaction to save changes
    except Exception as e:
        print(f"Error updating rich text for subreddit ID {subreddit_id}: {e}")



for start_idx in tqdm(range(0, len(subreddits_df), batch_size), desc="Preparing rich text"):
    end_idx = min(start_idx + batch_size, len(subreddits_df))

    # Get the current batch of subreddits
    subreddit_batch = subreddits_df.iloc[start_idx:end_idx]

    top_posts_batch = get_top_posts_batch(conn, subreddit_batch['id'].tolist(), limit=5)

    # Generate rich text for the current batch
    rich_text_batch = create_rich_text_for_batch(subreddit_batch, top_posts_batch)

    subreddits_df.loc[start_idx:end_idx-1, 'rich_text'] = rich_text_batch

    # Update the database with the generated rich text
    for idx, rich_text in zip(subreddit_batch['id'], rich_text_batch):
        update_rich_text_in_db(conn, idx, rich_text)

model = SentenceTransformer('all-MiniLM-L6-v2')
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Function to generate embeddings for rich text entries
def generate_embeddings_in_batches(texts, batch_size=32):
    embeddings = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Generating embeddings in batches"):
        batch_texts = texts[i:i+batch_size]
        batch_embeddings = model.encode(batch_texts, batch_size=batch_size, show_progress_bar=False, device=device)
        embeddings.append(batch_embeddings)
    return np.vstack(embeddings)

texts = subreddits_df['rich_text'].tolist()
embeddings = generate_embeddings_in_batches(texts)

"""## Building and Saving the FAISS Index"""

# Building FAISS Index
embeddings = embeddings.astype('float32')
embedding_dim = embeddings.shape[1]

# Initialize FAISS index for L2 distance
index = faiss.IndexFlatL2(embedding_dim)
index.add(embeddings)

# Mapping subreddit IDs to embeddings
id_to_subreddit = dict(enumerate(subreddits_df['id'].tolist()))
subreddit_id_to_details = subreddits_df.set_index('id').to_dict(orient='index')

# Save the FAISS index and mappings
faiss.write_index(index, "enriched_subreddit_faiss_index.bin")

with open('id_to_subreddit.pkl', 'wb') as f:
    pickle.dump(id_to_subreddit, f)

with open('subreddit_id_to_details.pkl', 'wb') as f:
    pickle.dump(subreddit_id_to_details, f)

# Save embeddings to a file
np.save("subreddit_embeddings.npy", embeddings)

print("Embeddings, FAISS index, and mappings saved successfully!")

# Execute the ALTER TABLE command to add the rich_text column
def add_rich_text_column(conn):
    try:
        conn.execute("ALTER TABLE subreddits ADD COLUMN rich_text TEXT;")
        conn.commit()  # Commit the changes to the database
        print("Column 'rich_text' added successfully.")
    except Exception as e:
        print(f"Error adding 'rich_text' column: {e}")

# Call the function to add the column
add_rich_text_column(conn)