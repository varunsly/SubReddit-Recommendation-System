# -*- coding: utf-8 -*-
"""SubredditTopCommunitiesScrapping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nKMsZlM1jPLDyZ7jk7MWN1A3i6AQUnAO
"""

pip install beautifulsoup4 requests

import asyncio
import asyncpraw
import json
import aiohttp
from asyncprawcore import RequestException, ResponseException
from datetime import datetime
import requests
from bs4 import BeautifulSoup

# Initialize the Reddit instance
reddit = asyncpraw.Reddit(# Initialize the Reddit instance
    client_id='zzukpp3wgMBgZQD5vXHPEg',
    client_secret='0fkYR_YUD26OF571VDmGlOneh6z8VQ',
    user_agent='subreddit scraper by u/Rude_Ice_5534'
)

# Function to fetch subreddit metadata
async def fetch_subreddit_metadata(subreddit):
    await subreddit.load()  # Ensure the subreddit object is fully loaded
    return {
        'name': subreddit.display_name,
        'description': subreddit.public_description,
        'subscribers': subreddit.subscribers,
        'created_utc': subreddit.created_utc
    }

# Function to fetch sample posts from a subreddit
async def fetch_sample_posts(subreddit, limit=10):
    posts = []
    async for post in subreddit.top(limit=limit):
        posts.append({
            'title': post.title,
            'selftext': post.selftext,
            'upvotes': post.ups,
            'downvotes': post.downs,
            'created_utc': post.created_utc,
            'author': post.author.name if post.author else None
        })
    return posts

# Function to fetch subreddit metadata with exponential backoff
async def fetch_with_backoff(fetch_func, *args, **kwargs):
    max_attempts = 5
    for attempt in range(max_attempts):
        try:
            return await fetch_func(*args, **kwargs)
        except ResponseException as e:
            if e.response.status_code == 429:
                sleep_time = 2 ** attempt
                print(f"Rate limited. Sleeping for {sleep_time} seconds.")
                await asyncio.sleep(sleep_time)
            else:
                raise
        except aiohttp.ClientError as e:
            sleep_time = 2 ** attempt
            print(f"Connection error: {e}. Retrying in {sleep_time} seconds.")
            await asyncio.sleep(sleep_time)
    raise Exception("Max retries exceeded")

# Save data to uniquely named JSON files
def save_data_to_json(metadata, posts, categories, count):
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    metadata_filename = f'subreddit_metadata_{timestamp}_{count}.json'
    posts_filename = f'subreddit_posts_{timestamp}_{count}.json'
    categories_filename = f'subreddit_categories_{timestamp}_{count}.json'

    with open(metadata_filename, 'w') as metadata_file:
        json.dump(metadata, metadata_file)

    with open(posts_filename, 'w') as posts_file:
        json.dump(posts, posts_file)

    with open(categories_filename, 'w') as categories_file:
        json.dump(categories, categories_file)

# Fetch all subreddits and collect metadata and sample posts
async def fetch_all_subreddits_and_data(subreddits, categories, limit_per_subreddit=10, save_interval=10):
    subreddit_metadata_list = []
    subreddit_posts_dict = {}
    count = 0

    for category, subs in subreddits.items():
        for subreddit_name in subs:
            try:
                subreddit = await reddit.subreddit(subreddit_name)
                # Fetch subreddit metadata
                metadata = await fetch_with_backoff(fetch_subreddit_metadata, subreddit)
                subreddit_metadata_list.append(metadata)

                # Fetch sample posts
                posts = await fetch_with_backoff(fetch_sample_posts, subreddit, limit=limit_per_subreddit)
                subreddit_posts_dict[subreddit.display_name] = posts

                count += 1

                # Save data periodically
                if count % save_interval == 0:
                    save_data_to_json(subreddit_metadata_list, subreddit_posts_dict, categories, count)
                    print(f"Data saved after processing {count} subreddits.")
                    # Reset the lists after saving
                    subreddit_metadata_list = []
                    subreddit_posts_dict = {}

                # To avoid rate limiting, you can add a delay
                await asyncio.sleep(1)  # Adjust the sleep time as necessary

            except Exception as e:
                print(f"Error fetching data for subreddit {subreddit_name}: {e}")

    # Final save after all subreddits are processed
    if subreddit_metadata_list or subreddit_posts_dict:
        save_data_to_json(subreddit_metadata_list, subreddit_posts_dict, categories, count)
        print("Final data saved.")

# Scrape subreddits and categories from the provided URLs
def scrape_subreddits_and_categories():
    urls = [
        "https://www.reddit.com/r/ListOfSubreddits/wiki/listofsubreddits/",
        "https://www.reddit.com/r/ListOfSubreddits/wiki/newtoreddit/"
    ]

    subreddits = {}
    categories = {}

    for url in urls:
        response = requests.get(url, headers={'User-agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(response.content, 'html.parser')

        for item in soup.select('ul ul > li'):
            category = item.find_previous('h2').text if item.find_previous('h2') else 'Uncategorized'
            subreddit_name = item.text.strip().split(' ')[0]

            if category not in subreddits:
                subreddits[category] = []
            subreddits[category].append(subreddit_name)

            if subreddit_name not in categories:
                categories[subreddit_name] = []
            categories[subreddit_name].append(category)

    return subreddits, categories

# Example usage
async def main():
    subreddits, categories = scrape_subreddits_and_categories()
    await fetch_all_subreddits_and_data(subreddits, categories, limit_per_subreddit=10, save_interval=10)

# Run the main function
asyncio.run(main())