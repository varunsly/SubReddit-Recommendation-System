# -*- coding: utf-8 -*-
"""Reddit_Data_API_Scrape.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vpVnXJWbdFE1oOnMumXwoKeiFbBXakU_

## Reddit Meta Data and Post Data Scrapping

### Install and Import Required Libraries
"""

# pip install praw pandas textblob vaderSentiment openai tqdm numpy asyncpraw

import asyncio
import asyncpraw
import json
import aiohttp
from asyncprawcore import RequestException, ResponseException
import praw
import asyncpraw
import nest_asyncio
import asyncio
import pandas as pd
from asyncpraw.models import MoreComments
import re
import string
import pandas as pd
from textblob import TextBlob
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Ensure NLTK stopwords and lemmatizer are downloaded
import nltk
# nltk.download('stopwords')
# nltk.download('punkt')
# nltk.download('wordnet')
import json
from datetime import datetime

"""### Initialize Reddit Instance
***We initialize the Reddit instance with our credentials.***


"""

reddit = asyncpraw.Reddit(
    client_id='#######',
    client_secret='#######',
    user_agent='###################'
)

nest_asyncio.apply()

"""### Reddit Subreddit Data Collection
We will collect data from Reddit using the asyncpraw library. Our goal is to fetch a comprehensive list of subreddits, gather metadata, and sample posts with comments for each subreddit.

a. **Fetch Subreddit Metadata:**
Collect metadata like subreddit names, descriptions, subscriber counts, and topics.

b. **Sample Posts:**
For each subreddit, fetch a limited number of top posts (e.g., top 10 or 20) to get a sense of the content and themes.
"""

# # Function to fetch subreddit metadata
# async def fetch_subreddit_metadata(subreddit):
#     await subreddit.load()  # Ensure the subreddit object is fully loaded
#     print(subreddit.display_name)
#     return {
#         'name': subreddit.display_name,
#         'description': subreddit.public_description,
#         'subscribers': subreddit.subscribers,
#         'created_utc': subreddit.created_utc
#     }

# # Function to fetch sample posts from a subreddit
# async def fetch_sample_posts(subreddit, limit=10):
#     posts = []
#     async for post in subreddit.top(limit=limit):
#         posts.append({
#             'title': post.title,
#             'selftext': post.selftext,
#             'upvotes': post.ups,
#             'downvotes': post.downs,
#             'created_utc': post.created_utc,
#             'author': post.author.name if post.author else None
#         })
#     return posts

# # Function to fetch comments for a post
# async def fetch_comments(post_id):
#     comments = []
#     submission = await reddit.submission(id=post_id)
#     await submission.comments.replace_more(limit=None)
#     for comment in submission.comments.list():
#         comments.append({
#             'body': comment.body,
#             'upvotes': comment.ups,
#             'downvotes': comment.downs,
#             'created_utc': comment.created_utc,
#             'author': comment.author.name if comment.author else None
#         })
#     return comments


# # Function to fetch subreddit metadata with exponential backoff
# async def fetch_with_backoff(fetch_func, *args, **kwargs):
#     max_attempts = 5
#     for attempt in range(max_attempts):
#         try:
#             return await fetch_func(*args, **kwargs)
#         except ResponseException as e:
#             if e.response.status_code == 429:
#                 sleep_time = 2 ** attempt
#                 print(f"Rate limited. Sleeping for {sleep_time} seconds.")
#                 await asyncio.sleep(sleep_time)
#             else:
#                 raise
#         except aiohttp.ClientError as e:
#             sleep_time = 2 ** attempt
#             print(f"Connection error: {e}. Retrying in {sleep_time} seconds.")
#             await asyncio.sleep(sleep_time)
#     raise Exception("Max retries exceeded")

# # Save data to uniquely named JSON files
# def save_data_to_json(metadata, posts, count):
#     timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
#     metadata_filename = f'subreddit_metadata_{timestamp}_{count}.json'
#     posts_filename = f'subreddit_posts_{timestamp}_{count}.json'

#     with open(metadata_filename, 'w') as metadata_file:
#         json.dump(metadata, metadata_file)

#     with open(posts_filename, 'w') as posts_file:
#         json.dump(posts, posts_file)

# # Save data to uniquely named JSON files

# # def save_data_to_json(metadata, posts, comments, count):
# #     timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
# #     metadata_filename = f'subreddit_metadata_{timestamp}_{count}.json'
# #     posts_filename = f'subreddit_posts_{timestamp}_{count}.json'
# #     comments_filename = f'subreddit_comments_{timestamp}_{count}.json'

# #     with open(metadata_filename, 'w') as metadata_file:
# #         json.dump(metadata, metadata_file)

# #     with open(posts_filename, 'w') as posts_file:
# #         json.dump(posts, posts_file)

# #     with open(comments_filename, 'w') as comments_file:
# #         json.dump(comments, comments_file)


# # # Fetch all subreddits and collect metadata, posts, and comments
# # async def fetch_all_subreddits_and_data(subreddit_names, category_dict, limit_per_subreddit=10, save_interval=10):
# #     subreddit_metadata_list = []
# #     subreddit_posts_dict = {}
# #     subreddit_comments_dict = {}
# #     count = 0

# #     for subreddit_name in subreddit_names:
# #         try:
# #             subreddit = await reddit.subreddit(subreddit_name)
# #             print(subreddit, subreddit_name)
# #             # Fetch subreddit metadata
# #             metadata = await fetch_with_backoff(fetch_subreddit_metadata, subreddit)
# #             metadata['category'] = category_dict.get(subreddit_name, 'Unknown')
# #             subreddit_metadata_list.append(metadata)

# #             # Fetch sample posts
# #             posts = await fetch_with_backoff(fetch_sample_posts, subreddit, limit=limit_per_subreddit)
# #             subreddit_posts_dict[subreddit_name] = posts

# #             # Fetch comments for each post
# #             comments = []
# #             for post in posts:
# #                 post_comments = await fetch_with_backoff(fetch_comments, post['id'])
# #                 comments.extend(post_comments)
# #             subreddit_comments_dict[subreddit_name] = comments

# #             count += 1

# #             # Save data periodically
# #             if count % save_interval == 0:
# #                 save_data_to_json(subreddit_metadata_list, subreddit_posts_dict, subreddit_comments_dict, count)
# #                 print(f"Data saved after processing {count} subreddits.")
# #                 subreddit_metadata_list = []
# #                 subreddit_posts_dict = {}
# #                 subreddit_comments_dict = {}

# #             await asyncio.sleep(1)  # Adjust the sleep time as necessary

# #         except Exception as e:
# #             print(f"Error fetching data for subreddit {subreddit_name}: {e}")

# #     # Final save after all subreddits are processed
# #     if subreddit_metadata_list or subreddit_posts_dict or subreddit_comments_dict:
# #         save_data_to_json(subreddit_metadata_list, subreddit_posts_dict, subreddit_comments_dict, count)
# #         print("Final data saved.")


# # Fetch all subreddits and collect metadata and sample posts
# async def fetch_all_subreddits_and_data(limit_per_subreddit=10, save_interval=10):
#     subreddit_metadata_list = []
#     subreddit_posts_dict = {}
#     count = 0

#     async for subreddit in reddit.subreddits.new(limit=None):  # Paginate through all new subreddits
#         try:
#             # Fetch subreddit metadata
#             print(subreddit)
#             metadata = await fetch_with_backoff(fetch_subreddit_metadata, subreddit)
#             subreddit_metadata_list.append(metadata)

#             # Fetch sample posts
#             posts = await fetch_with_backoff(fetch_sample_posts, subreddit, limit=limit_per_subreddit)
#             subreddit_posts_dict[subreddit.display_name] = posts

#             count += 1

#             # Save data periodically
#             if count % save_interval == 0:
#                 save_data_to_json(subreddit_metadata_list, subreddit_posts_dict, count)
#                 print(f"Data saved after processing {count} subreddits.")
#                 # Reset the lists after saving
#                 subreddit_metadata_list = []
#                 subreddit_posts_dict = {}

#             # To avoid rate limiting, you can add a delay
#             await asyncio.sleep(1)  # Adjust the sleep time as necessary

#         except Exception as e:
#             print(f"Error fetching data for subreddit {subreddit.display_name}: {e}")

#     # Final save after all subreddits are processed
#     if subreddit_metadata_list or subreddit_posts_dict:
#         save_data_to_json(subreddit_metadata_list, subreddit_posts_dict, count)
#         print("Final data saved.")

# # Example usage
# async def main():
#     await fetch_all_subreddits_and_data(limit_per_subreddit=10, save_interval=10)

# # Run the main function
# asyncio.run(main())

"""### Fetch Subreddit Metadata"""

# Function to fetch subreddit metadata
async def fetch_subreddit_metadata(subreddit):
    await subreddit.load()  # Ensure the subreddit object is fully loaded
    # print(subreddit.display_name)
    return {
        'name': subreddit.display_name,
        'description': subreddit.public_description,
        'subscribers': subreddit.subscribers,
        'created_utc': subreddit.created_utc
    }

"""### Fetch Top Subreddits"""

# Function to fetch sample posts from a subreddit
async def fetch_sample_posts(subreddit, limit=10):
    posts = []
    async for post in subreddit.top(limit=limit):
        posts.append({
            'id': post.id,
            'title': post.title,
            'selftext': post.selftext,
            'upvotes': post.ups,
            'downvotes': post.downs,
            'created_utc': post.created_utc,
            'author': post.author.name if post.author else None
        })
    return posts

"""### Fetch top 20 comments for a post"""

# # # Function to fetch top 20 comments for a post
# # async def fetch_comments(post_id):
# # Function to fetch top 10 comments for a post
# async def fetch_comments(post_id, limit=10):
#     submission = await reddit.submission(id=post_id)
#     await submission.load()

#     # Sort comments by upvotes and limit to the top 'limit' comments
#     top_comments = []

#     submission.comment_sort = 'top'
#     submission.comment_limit = limit

#     await submission.comments.replace_more(limit=None)
#     comments = submission.comments.list()

#     for comment in comments[:limit]:
#         top_comments.append({
#             'body': comment.body,
#             'upvotes': comment.ups,
#             'downvotes': comment.downs,
#             'created_utc': comment.created_utc,
#             'author': comment.author.name if comment.author else None
#         })

#     return top_comments

"""### Fetch data with exponential backoff"""

# Function to fetch subreddit metadata with exponential backoff
async def fetch_with_backoff(fetch_func, *args, **kwargs):
    max_attempts = 5
    for attempt in range(max_attempts):
        try:
            return await fetch_func(*args, **kwargs)
        except ResponseException as e:
            if e.response.status_code == 429:
                sleep_time = 2 ** attempt
                print(f"Rate limited. Sleeping for {sleep_time} seconds.")
                await asyncio.sleep(sleep_time)
            else:
                raise
        except aiohttp.ClientError as e:
            sleep_time = 2 ** attempt
            print(f"Connection error: {e}. Retrying in {sleep_time} seconds.")
            await asyncio.sleep(sleep_time)
    raise Exception("Max retries exceeded")

"""### Save data to uniquely named JSON files"""

# Save data to uniquely named JSON files
# def save_data_to_json(metadata, posts, comments, count):
#     timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
#     metadata_filename = f'subreddit_metadata_{timestamp}_{count}.json'
#     posts_filename = f'subreddit_posts_{timestamp}_{count}.json'
#     comments_filename = f'subreddit_comments_{timestamp}_{count}.json'

#     with open(metadata_filename, 'w') as metadata_file:
#         json.dump(metadata, metadata_file)

#     with open(posts_filename, 'w') as posts_file:
#         json.dump(posts, posts_file)

#     with open(comments_filename, 'w') as comments_file:
#         json.dump(comments, comments_file)


# Save data to uniquely named JSON files
def save_data_to_json(metadata, count):
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    metadata_filename = f'subreddit_metadata_{timestamp}_{count}.json'
    # posts_filename = f'subreddit_posts_{timestamp}_{count}.json'

    with open(metadata_filename, 'w') as metadata_file:
        json.dump(metadata, metadata_file)

    # with open(posts_filename, 'w') as posts_file:
    #     json.dump(posts, posts_file)

"""### Fetch all subreddits and collect metadata, posts, and comments"""

# Fetch all subreddits and collect metadata, posts, and comments
async def fetch_all_subreddits_and_data(subreddit_names, category_dict, limit_per_subreddit=10, save_interval=100):
    subreddit_metadata_list = []
    subreddit_posts_dict = {}
    subreddit_comments_dict = {}
    count = 0

    for subreddit_name in subreddit_names:
        try:
            subreddit_name = subreddit_name.replace('r/', '')
            # print(f" subreddit {subreddit_name}")
            subreddit = await reddit.subreddit(subreddit_name)

            # print(f"Fetching data for subreddit {subreddit}")

            # Fetch subreddit metadata
            metadata = await fetch_with_backoff(fetch_subreddit_metadata, subreddit)
            metadata['category'] = category_dict.get(subreddit_name, 'Unknown')
            subreddit_metadata_list.append(metadata)

            # Fetch sample posts
            # posts = await fetch_with_backoff(fetch_sample_posts, subreddit, limit=limit_per_subreddit)
            # subreddit_posts_dict[subreddit_name] = posts

            # # # Fetch comments for each post
            # comments = []
            # for post in posts:
            #     post_comments = await fetch_with_backoff(fetch_comments, post['id'])
            #     comments.extend(post_comments)
            # subreddit_comments_dict[subreddit_name] = comments

            count += 1

            # # Save data periodically
            # if count % save_interval == 0:
            #     save_data_to_json(subreddit_metadata_list, subreddit_posts_dict, subreddit_comments_dict, count)
            #     print(f"Data saved after processing {count} subreddits.")
            #     subreddit_metadata_list = []
            #     subreddit_posts_dict = {}
            #     subreddit_comments_dict = {}




            # Save data periodically
            if count % save_interval == 0:
                # save_data_to_json(subreddit_metadata_list, subreddit_posts_dict, count)
                save_data_to_json(subreddit_metadata_list, count)

                print(f"Data saved after processing {count} subreddits.")
                subreddit_metadata_list = []
                subreddit_posts_dict = {}
                subreddit_comments_dict = {}


            await asyncio.sleep(1)  # Adjust the sleep time as necessary

        except Exception as e:
            print(f"Error fetching data for subreddit {subreddit_name}: {e}")

    # # Final save after all subreddits are processed
    # if subreddit_metadata_list or subreddit_posts_dict or subreddit_comments_dict:
    #     save_data_to_json(subreddit_metadata_list, subreddit_posts_dict,subreddit_comments_dict, count)
    #     print("Final data saved.")
    # # Final save after all subreddits are processed


    if subreddit_metadata_list or subreddit_posts_dict:
        save_data_to_json(subreddit_metadata_list, subreddit_posts_dict, count)
        print("Final data saved.")

"""### Main Function"""

async def main():
    # Read subreddit names and categories from CSV
    df = pd.read_csv('consoidated_reddit_communities.csv')
    subreddit_names = df['Subreddit'][250744:].tolist()
    category_dict = dict(zip(df['Subreddit'], df['Type']))
    # print(df.head())
    await fetch_all_subreddits_and_data(subreddit_names, category_dict, limit_per_subreddit=10, save_interval=100)

# Run the main function
asyncio.run(main())

"""### Data Merging"""

import pandas as pd
import glob
import json

# Consolidate Metadata Files into a DataFrame
metadata_files_pattern = 'subreddit_metadata_*.json'
metadata_files = glob.glob(metadata_files_pattern)
metadata_dfs = []

for file in metadata_files:
    with open(file, 'r') as f:
        data = json.load(f)
        df = pd.DataFrame(data)
        metadata_dfs.append(df)

metadata_df = pd.concat(metadata_dfs, ignore_index=True)
metadata_df.to_csv('consolidated_metadata_rest.csv', index=False)
print("Metadata consolidated into 'consolidated_metadata1.csv'")

# # Consolidate Post Files into a DataFrame
# posts_files_pattern = 'subreddit_posts_*.json'
# posts_files = glob.glob(posts_files_pattern)
# posts_dfs = []

# for file in posts_files:
#     with open(file, 'r') as f:
#         data = json.load(f)
#         for subreddit, posts in data.items():
#             df = pd.DataFrame(posts)
#             df['subreddit'] = subreddit
#             posts_dfs.append(df)

# posts_df = pd.concat(posts_dfs, ignore_index=True)
# posts_df.to_csv('consolidated_posts.csv', index=False)
# print("Posts consolidated into 'consolidated_posts.csv'")

